# Reviewing Policy Gradient methods

>Continuous states and actions in high dimensional spaces cannot be treated by most off-the-shelf reinforcement learning approaches. Policy gradient methods differ significantly as they do not suffer from these problems in the same way. For example, uncertainty in the state might degrade the performance of the policy (if no additional state estimator is being used) but the optimization techniques for the policy do not need to be changed. Continuous states and actions can be dealt with in exactly the same way as discrete ones while, in addition, the learning performance is often increased. Convergence at least to a local optimum is guaranteed (**critical for robotics**).

>The advantages of policy gradient methods for real world applications are numerous. Among the most important ones are that the policy representations can be chosen so that it is meaningful for the task and can incorporate domain knowledge, that often fewer parameters are needed in the learning process than in value-function based approaches and that there is a variety of different algorithms for policy gradient estimation in the literature which have a rather strong theoretical underpinning. Additionally, policy gradient methods can be used either model-free or model-based as they are a generic formulation.

Policy gradient methods however,
> are by definition on-policy (note that tricks like importance sampling can slightly alleviate this problem) and need to forget data very fast in order to avoid the introduction of a bias to the gradient estimator. Hence, the use of sampled data is not very efficient. In tabular representations, value function methods are guaranteed to converge to a global maximum while policy gradients only converge to a local maximum and there may be many maxima in discrete problems. Policy gradient methods are often quite demanding to apply, mainly because one has to have considerable knowledge about the system one wants to control to make reasonable policy definitions. Finally, policy gradient methods always have an open parameter, the learning rate, which may decide over the order of magnitude of the speed of convergence, these have led to new approaches inspired by expectation-maximization (see, e.g., Vlassis et al., 2009; Kober & Peters, 2008).


### Notation:
The three main components of a Reinfocement Learning (RL) system for robotics include the state <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/6f9bad7347b91ceebebd3ad7e6f6f2d1.svg?invert_in_darkmode" align=middle width=7.705549500000004pt height=14.155350000000013pt/> (also found in literature as <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/>), the action <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/44bc9d542a92714cac84e01cbbb7fd61.svg?invert_in_darkmode" align=middle width=8.689230000000004pt height=14.155350000000013pt/> (also found as <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/6dbb78540bd76da3f1625782d42d6d16.svg?invert_in_darkmode" align=middle width=9.410280000000004pt height=14.155350000000013pt/>) and the reward denoted by <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/89f2e0d2d24bcf44db73aab8fc03252c.svg?invert_in_darkmode" align=middle width=7.873024500000003pt height=14.155350000000013pt/>. The stochasticity of the environment gets represented by using a probability distribution <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/50099eadec837b3ea3a1e32576adbb9e.svg?invert_in_darkmode" align=middle width=159.496755pt height=24.65759999999998pt/> as model where <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/687ec27e46c5b07ba8adaab70976974e.svg?invert_in_darkmode" align=middle width=63.0102pt height=27.656969999999987pt/> denotes the current action and <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/59efeb0f4f5d484a9b8a404d5bdac544.svg?invert_in_darkmode" align=middle width=14.971605000000004pt height=14.155350000000013pt/>, <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/eaa3f599744549c4805d08b50d0b9d3d.svg?invert_in_darkmode" align=middle width=75.79803pt height=27.656969999999987pt/> denote the current and next state, respectively. Further, we assume that actions are generated by a policy <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/e2f0dada90b45ef6dd58f15372396f59.svg?invert_in_darkmode" align=middle width=113.628075pt height=24.65759999999998pt/> which is modeled as a probability distribution in order to incorporate exploratory actions. The policy is assumed to be parameterized by <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/d6328eaebbcd5c358f426dbea4bdbf70.svg?invert_in_darkmode" align=middle width=15.137100000000004pt height=22.46574pt/> policy parameters <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/2b37baa87099988c8cd4a6844baa66ff.svg?invert_in_darkmode" align=middle width=61.029705pt height=27.656969999999987pt/>. The sequence of states and actions forms a trajectory denoted by <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/85e4a233f33b0a54f2020db13ef59cc6.svg?invert_in_darkmode" align=middle width=109.53294pt height=24.65759999999998pt/> where <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/7b9a0316a2fcd7f01cfd556eedf72e96.svg?invert_in_darkmode" align=middle width=14.999985000000004pt height=22.46574pt/> denoted the horizon which can be infinite. Often, *trajectory*, *history*, *trial* or *roll-out* are used interchangeably. At each instant of time, the learning system receives a reward <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/d18b0f40de3e43b4fd5efdfec3abebff.svg?invert_in_darkmode" align=middle width=141.130275pt height=24.65759999999998pt/>.

The general goal of policy optimization in reinforcement learning for robots is to optimize the policy parameters <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/1995b01bb3050d9d9821e07ddcbe14e8.svg?invert_in_darkmode" align=middle width=51.988530000000004pt height=27.656969999999987pt/> so that the expected return:
<p align="center"><img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/0b1e60f204bede37b3c9cb95595b38ee.svg?invert_in_darkmode" align=middle width=172.9365pt height=39.45249pt/></p>

is optimized where <img src="https://rawgit.com/vmayoral/basic_reinforcement_learning/master//tutorial12/tex/11c596de17c342edeed29f489aa4b274.svg?invert_in_darkmode" align=middle width=9.423975000000004pt height=14.155350000000013pt/> becomes


# Sources:
- http://www.scholarpedia.org/article/Policy_gradient_methods#Likelihood_Ratio_Methods_and_REINFORCE
- https://theneuralperspective.com/2016/10/27/gradient-topics/
- https://theneuralperspective.com/2016/11/25/reinforcement-learning-rl-policy-gradients-i/
- https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient
