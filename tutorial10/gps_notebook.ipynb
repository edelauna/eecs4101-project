{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Policy Search\n",
    "\n",
    "<h2 id=\"tocheading\">Index</h2>\n",
    "<div id=\"toc\"></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Guided Policy Search (GPS) is a technique that transforms the Reinforcement Learning (RL) task of policy search into a Supervised Learning problem, where the training set is generated by a simple trajectory-centric RL algorithm. \n",
    "\n",
    "This algorithm optimizes linear-Gaussian controllers $p_i (u_t | x_t)$. Each $p_i (u_t | x_t)$ succeeds in the task from different initial states which helps the algorithm to generalize to other states from the same distribution. The final policy $\\pi_\\theta(u_t | o_t )$ learned with GPS is only provided with observations $o_t$ of the full state $x_t$, and assumed dynamics are assumed to be unknown. \n",
    "\n",
    "![](gps_illustration.png)\n",
    "\n",
    "We draw sample trajectories $\\tau_i^j$ for each initial state on the physical system by running the corresponding controller $p_i(u_t | x_t)$. The samples are used to fit the dynamics $p_i (x_{t+1} | x_t, u_t)$ that are used to improve the controllers $p_i(u_t | x_t)$, and serve as training data for the policy $\\pi_\\theta(u_t | o_t )$. Within the graph we can observe how there's a loop that alternates between optimizing each trajectory distribution $p_i (\\tau)$ and optimizing the policy $\\pi_\\theta(u_t | o_t )$ to match these trajectory distributions.\n",
    "\n",
    "\n",
    "This work is based on https://arxiv.org/abs/1504.00702. Refer to http://rll.berkeley.edu/gps/ for the original implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions and notations\n",
    "\n",
    "| corresponding symbol | definition |\n",
    "|--------|------------|\n",
    "| $p_i(u_t | x_t)$ | linear-Gaussian controllers, they induce the trajectory distributions $p_i (\\tau)$ |\n",
    "| $\\hat{p_i}(u_t | x_t)$ | previous controllers, previous time step t-1 |\n",
    "| $\\pi_\\theta(u_t | o_t )$| final policy learned |\n",
    "| $p_i (\\tau)$| trajectory distribution induced from the linear-Gaussian controllers, guiding distribution |\n",
    "| $\\tau_i^j$ | sample trajectories, sampled from the distribution |\n",
    "| $o_t$ | observations |\n",
    "| $x_t$ | full state |\n",
    "| $p_i (x_{t+1} | x_t, u_t)$ | system dynamics |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test environment\n",
    "The following test environment will be used for the purpose of implementing GPS.\n",
    "\n",
    "![](gps_testenv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-17 12:43:38,300] Making new env: Pendulum-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPS implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils\n",
    "A set of utility functions used along the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gauss_fit_joint_prior(pts, mu0, Phi, m, n0, dwts, dX, dU, sig_reg):\n",
    "    \"\"\" Perform Gaussian fit to data with a prior. \"\"\"\n",
    "    # Build weights matrix.\n",
    "    D = np.diag(dwts)\n",
    "    # Compute empirical mean and covariance.\n",
    "    mun = np.sum((pts.T * dwts).T, axis=0)\n",
    "    diff = pts - mun\n",
    "    empsig = diff.T.dot(D).dot(diff)\n",
    "    empsig = 0.5 * (empsig + empsig.T)\n",
    "    # MAP estimate of joint distribution.\n",
    "    N = dwts.shape[0]\n",
    "    mu = mun\n",
    "    sigma = (N * empsig + Phi + (N * m) / (N + m) *\n",
    "             np.outer(mun - mu0, mun - mu0)) / (N + n0)\n",
    "    sigma = 0.5 * (sigma + sigma.T)\n",
    "    # Add sigma regularization.\n",
    "    sigma += sig_reg\n",
    "    # Conditioning to get dynamics.\n",
    "    fd = np.linalg.solve(sigma[:dX, :dX], sigma[:dX, dX:dX+dU]).T\n",
    "    fc = mu[dX:dX+dU] - fd.dot(mu[:dX])\n",
    "    dynsig = sigma[dX:dX+dU, dX:dX+dU] - fd.dot(sigma[:dX, :dX]).dot(fd.T)\n",
    "    dynsig = 0.5 * (dynsig + dynsig.T)\n",
    "    return fd, fc, dynsig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamics\n",
    "#### Dynamics superclass: `Dynamics`\n",
    "The dynamical model superclass which assumes dynamics are always linear with $x_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Dynamics(object):\n",
    "    \"\"\" Dynamics superclass. \"\"\"\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    def __init__(self, hyperparams):\n",
    "    #def __init__(self):\n",
    "        self._hyperparams = hyperparams\n",
    "\n",
    "        # TODO - Currently assuming that dynamics will always be linear\n",
    "        #        with X.\n",
    "        # TODO - Allocate arrays using hyperparams dU, dX, T.\n",
    "\n",
    "        # Fitted dynamics: x_t+1 = Fm * [x_t;u_t] + fv.\n",
    "        self.Fm = np.array(np.nan)\n",
    "        self.fv = np.array(np.nan)\n",
    "        self.dyn_covar = np.array(np.nan)  # Covariance.\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update_prior(self, sample):\n",
    "        \"\"\" Update dynamics prior. \"\"\"\n",
    "        raise NotImplementedError(\"Must be implemented in subclass.\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_prior(self):\n",
    "        \"\"\" Returns prior object. \"\"\"\n",
    "        raise NotImplementedError(\"Must be implemented in subclass.\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, sample_list):\n",
    "        \"\"\" Fit dynamics. \"\"\"\n",
    "        raise NotImplementedError(\"Must be implemented in subclass.\")\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\" Return a copy of the dynamics estimate. \"\"\"\n",
    "        dyn = type(self)(self._hyperparams)\n",
    "        #dyn = type(self)()\n",
    "        dyn.Fm = np.copy(self.Fm)\n",
    "        dyn.fv = np.copy(self.fv)\n",
    "        dyn.dyn_covar = np.copy(self.dyn_covar)\n",
    "        return dyn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian mixture model (GMM) class: `GMM`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the Gaussian mixture model (GMM) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import scipy.linalg\n",
    "\n",
    "def logsum(vec, axis=0, keepdims=True):\n",
    "    #TODO: Add a docstring.\n",
    "    maxv = np.max(vec, axis=axis, keepdims=keepdims)\n",
    "    maxv[maxv == -float('inf')] = 0\n",
    "    return np.log(np.sum(np.exp(vec-maxv), axis=axis, keepdims=keepdims)) + maxv\n",
    "\n",
    "class GMM(object):\n",
    "    \"\"\" Gaussian Mixture Model. \"\"\"\n",
    "    def __init__(self, init_sequential=False, eigreg=False, warmstart=True):\n",
    "        self.init_sequential = init_sequential\n",
    "        self.eigreg = eigreg\n",
    "        self.warmstart = warmstart\n",
    "        self.sigma = None\n",
    "\n",
    "    def inference(self, pts):\n",
    "        \"\"\"\n",
    "        Evaluate dynamics prior.\n",
    "        Args:\n",
    "            pts: A N x D array of points.\n",
    "        \"\"\"\n",
    "        # Compute posterior cluster weights.\n",
    "        logwts = self.clusterwts(pts)\n",
    "\n",
    "        # Compute posterior mean and covariance.\n",
    "        mu0, Phi = self.moments(logwts)\n",
    "\n",
    "        # Set hyperparameters.\n",
    "        m = self.N\n",
    "        n0 = m - 2 - mu0.shape[0]\n",
    "\n",
    "        # Normalize.\n",
    "        m = float(m) / self.N\n",
    "        n0 = float(n0) / self.N\n",
    "        return mu0, Phi, m, n0\n",
    "\n",
    "    def estep(self, data):\n",
    "        \"\"\"\n",
    "        Compute log observation probabilities under GMM.\n",
    "        Args:\n",
    "            data: A N x D array of points.\n",
    "        Returns:\n",
    "            logobs: A N x K array of log probabilities (for each point\n",
    "                on each cluster).\n",
    "        \"\"\"\n",
    "        # Constants.\n",
    "        N, D = data.shape\n",
    "        K = self.sigma.shape[0]\n",
    "\n",
    "        logobs = -0.5*np.ones((N, K))*D*np.log(2*np.pi)\n",
    "        for i in range(K):\n",
    "            mu, sigma = self.mu[i], self.sigma[i]\n",
    "            L = scipy.linalg.cholesky(sigma, lower=True)\n",
    "            logobs[:, i] -= np.sum(np.log(np.diag(L)))\n",
    "\n",
    "            diff = (data - mu).T\n",
    "            soln = scipy.linalg.solve_triangular(L, diff, lower=True)\n",
    "            logobs[:, i] -= 0.5*np.sum(soln**2, axis=0)\n",
    "\n",
    "        logobs += self.logmass.T\n",
    "        return logobs\n",
    "\n",
    "    def moments(self, logwts):\n",
    "        \"\"\"\n",
    "        Compute the moments of the cluster mixture with logwts.\n",
    "        Args:\n",
    "            logwts: A K x 1 array of log cluster probabilities.\n",
    "        Returns:\n",
    "            mu: A (D,) mean vector.\n",
    "            sigma: A D x D covariance matrix.\n",
    "        \"\"\"\n",
    "        # Exponentiate.\n",
    "        wts = np.exp(logwts)\n",
    "\n",
    "        # Compute overall mean.\n",
    "        mu = np.sum(self.mu * wts, axis=0)\n",
    "\n",
    "        # Compute overall covariance.\n",
    "        diff = self.mu - np.expand_dims(mu, axis=0)\n",
    "        diff_expand = np.expand_dims(self.mu, axis=1) * \\\n",
    "                np.expand_dims(diff, axis=2)\n",
    "        wts_expand = np.expand_dims(wts, axis=2)\n",
    "        sigma = np.sum((self.sigma + diff_expand) * wts_expand, axis=0)\n",
    "        return mu, sigma\n",
    "\n",
    "    def clusterwts(self, data):\n",
    "        \"\"\"\n",
    "        Compute cluster weights for specified points under GMM.\n",
    "        Args:\n",
    "            data: An N x D array of points\n",
    "        Returns:\n",
    "            A K x 1 array of average cluster log probabilities.\n",
    "        \"\"\"\n",
    "        # Compute probability of each point under each cluster.\n",
    "        logobs = self.estep(data)\n",
    "\n",
    "        # Renormalize to get cluster weights.\n",
    "        logwts = logobs - logsum(logobs, axis=1)\n",
    "\n",
    "        # Average the cluster probabilities.\n",
    "        logwts = logsum(logwts, axis=0) - np.log(data.shape[0])\n",
    "        return logwts.T\n",
    "\n",
    "    def update(self, data, K, max_iterations=100):\n",
    "        \"\"\"\n",
    "        Run EM to update clusters.\n",
    "        Args:\n",
    "            data: An N x D data matrix, where N = number of data points.\n",
    "            K: Number of clusters to use.\n",
    "        \"\"\"\n",
    "        # Constants.\n",
    "        N = data.shape[0]\n",
    "        Do = data.shape[1]\n",
    "\n",
    "        LOGGER.debug('Fitting GMM with %d clusters on %d points', K, N)\n",
    "\n",
    "        if (not self.warmstart or self.sigma is None or\n",
    "                K != self.sigma.shape[0]):\n",
    "            # Initialization.\n",
    "            LOGGER.debug('Initializing GMM.')\n",
    "            self.sigma = np.zeros((K, Do, Do))\n",
    "            self.mu = np.zeros((K, Do))\n",
    "            self.logmass = np.log(1.0 / K) * np.ones((K, 1))\n",
    "            self.mass = (1.0 / K) * np.ones((K, 1))\n",
    "            self.N = data.shape[0]\n",
    "            N = self.N\n",
    "\n",
    "            # Set initial cluster indices.\n",
    "            if not self.init_sequential:\n",
    "                cidx = np.random.randint(0, K, size=(1, N))\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            # Initialize.\n",
    "            for i in range(K):\n",
    "                cluster_idx = (cidx == i)[0]\n",
    "                mu = np.mean(data[cluster_idx, :], axis=0)\n",
    "                diff = (data[cluster_idx, :] - mu).T\n",
    "                sigma = (1.0 / K) * (diff.dot(diff.T))\n",
    "                self.mu[i, :] = mu\n",
    "                self.sigma[i, :, :] = sigma + np.eye(Do) * 2e-6\n",
    "\n",
    "        prevll = -float('inf')\n",
    "        for itr in range(max_iterations):\n",
    "            # E-step: compute cluster probabilities.\n",
    "            logobs = self.estep(data)\n",
    "\n",
    "            # Compute log-likelihood.\n",
    "            ll = np.sum(logsum(logobs, axis=1))\n",
    "            LOGGER.debug('GMM itr %d/%d. Log likelihood: %f',\n",
    "                         itr, max_iterations, ll)\n",
    "            if ll < prevll:\n",
    "                # TODO: Why does log-likelihood decrease sometimes?\n",
    "                LOGGER.debug('Log-likelihood decreased! Ending on itr=%d/%d',\n",
    "                             itr, max_iterations)\n",
    "                break\n",
    "            if np.abs(ll-prevll) < 1e-5*prevll:\n",
    "                LOGGER.debug('GMM converged on itr=%d/%d',\n",
    "                             itr, max_iterations)\n",
    "                break\n",
    "            prevll = ll\n",
    "\n",
    "            # Renormalize to get cluster weights.\n",
    "            logw = logobs - logsum(logobs, axis=1)\n",
    "            assert logw.shape == (N, K)\n",
    "\n",
    "            # Renormalize again to get weights for refitting clusters.\n",
    "            logwn = logw - logsum(logw, axis=0)\n",
    "            assert logwn.shape == (N, K)\n",
    "            w = np.exp(logwn)\n",
    "\n",
    "            # M-step: update clusters.\n",
    "            # Fit cluster mass.\n",
    "            self.logmass = logsum(logw, axis=0).T\n",
    "            self.logmass = self.logmass - logsum(self.logmass, axis=0)\n",
    "            assert self.logmass.shape == (K, 1)\n",
    "            self.mass = np.exp(self.logmass)\n",
    "            # Reboot small clusters.\n",
    "            w[:, (self.mass < (1.0 / K) * 1e-4)[:, 0]] = 1.0 / N\n",
    "            # Fit cluster means.\n",
    "            w_expand = np.expand_dims(w, axis=2)\n",
    "            data_expand = np.expand_dims(data, axis=1)\n",
    "            self.mu = np.sum(w_expand * data_expand, axis=0)\n",
    "            # Fit covariances.\n",
    "            wdata = data_expand * np.sqrt(w_expand)\n",
    "            assert wdata.shape == (N, K, Do)\n",
    "            for i in range(K):\n",
    "                # Compute weighted outer product.\n",
    "                XX = wdata[:, i, :].T.dot(wdata[:, i, :])\n",
    "                mu = self.mu[i, :]\n",
    "                self.sigma[i, :, :] = XX - np.outer(mu, mu)\n",
    "\n",
    "                if self.eigreg:  # Use eigenvalue regularization.\n",
    "                    raise NotImplementedError()\n",
    "                else:  # Use quick and dirty regularization.\n",
    "                    sigma = self.sigma[i, :, :]\n",
    "                    self.sigma[i, :, :] = 0.5 * (sigma + sigma.T) + \\\n",
    "                            1e-6 * np.eye(Do)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian mixture model (GMM) dynamics: `DynamicsPriorGMM`\n",
    "\n",
    "Optimizing the linear-Gaussian controllers $p_i(u_t | x_t)$ (that induce the trajectories $p_i (\\tau)$) requires fitting the system dynamics $p_i (x_{t+1} | x_t, u_t)$ at each iteration to samples generated on the physical system from the previous controller $\\hat{p_i}(u_t | x_t)$.\n",
    "\n",
    "![](trajectory_optimization.png)\n",
    "\n",
    "The linear-Gaussian dynamics are defined as $p_i (x_{t+1} | x_t, u_t) = \\mathcal{N} (f_{xt}x_t + f_{ut}u_t + f_{ct}, F_t)$, and the data that we obtain from the robot can be viewed as tuples $\\{x_t^i, u_t^i, x_{t+1}^i\\}$. A simple way to fit these linear-Gaussian dynamics is to use linear regression to determine $f_x$, $f_u$ and $f_c$, and fit $F_t$ based on errors however the sample complexity of linear regression scales with the dimensionality of the full state space $x_t$.\n",
    "\n",
    "Although this might be an issue for high-dimensional robotic systems, we can observe that the dynamics at nearby time steps are strongly correlated which means that we can dramatically reduce the sample complexity of the dynamics fitting by bringing in information from previous time steps. This implementation will fit a global model to all of the transitions $\\{x_t^i, u_t^i, x_{t+1}^i\\}$ for all t and all tuples from prior iterations and then use this model as **a prior for fitting the dynamics at each time step**.\n",
    "\n",
    "Below, the definition of GMM prior for dynamics estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# DynamicsPriorGMM\n",
    "DYN_PRIOR_GMM = {\n",
    "    'min_samples_per_cluster': 20,\n",
    "    'max_clusters': 50,\n",
    "    'max_samples': 20,\n",
    "    'strength': 1.0,\n",
    "}\n",
    "\n",
    "# As defined in the code examples\n",
    "DYN_PRIOR_GMM_example = {\n",
    "    'min_samples_per_cluster': 40,\n",
    "    'max_clusters': 20,\n",
    "    'max_samples': 20,    \n",
    "    'strength': 1.0,\n",
    "}\n",
    "\n",
    "class DynamicsPriorGMM(object):\n",
    "    \"\"\"\n",
    "    A dynamics prior encoded as a GMM over [x_t, u_t, x_t+1] points.\n",
    "    See:\n",
    "        S. Levine*, C. Finn*, T. Darrell, P. Abbeel, \"End-to-end\n",
    "        training of Deep Visuomotor Policies\", arXiv:1504.00702,\n",
    "        Appendix A.3.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Hyperparameters:\n",
    "            min_samples_per_cluster: Minimum samples per cluster.\n",
    "            max_clusters: Maximum number of clusters to fit.\n",
    "            max_samples: Maximum number of trajectories to use for\n",
    "                fitting the GMM at any given time.\n",
    "            strength: Adjusts the strength of the prior.\n",
    "        \"\"\"\n",
    "        config = copy.deepcopy(DYN_PRIOR_GMM)\n",
    "        #config = copy.deepcopy(DYN_PRIOR_GMM_example)        \n",
    "        config.update(hyperparams)\n",
    "        self._hyperparams = config\n",
    "        self.X = None\n",
    "        self.U = None\n",
    "        self.gmm = GMM()\n",
    "        self._min_samp = self._hyperparams['min_samples_per_cluster']\n",
    "        self._max_samples = self._hyperparams['max_samples']\n",
    "        self._max_clusters = self._hyperparams['max_clusters']\n",
    "        self._strength = self._hyperparams['strength']\n",
    "\n",
    "        # Should we use copy.min_samples_per_cluster, etc. instead?\n",
    "        #self._min_samp = DYN_PRIOR_GMM_example.min_samples_per_cluster\n",
    "        #self._max_samples = DYN_PRIOR_GMM_example.max_samples\n",
    "        #self._max_clusters = DYN_PRIOR_GMM_example.max_clusters\n",
    "        #self._strength = DYN_PRIOR_GMM_example.strength\n",
    "        \n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\" Return dynamics prior for initial time step. \"\"\"\n",
    "        # Compute mean and covariance.\n",
    "        mu0 = np.mean(self.X[:, 0, :], axis=0)\n",
    "        Phi = np.diag(np.var(self.X[:, 0, :], axis=0))\n",
    "\n",
    "        # Factor in multiplier.\n",
    "        n0 = self.X.shape[2] * self._strength\n",
    "        m = self.X.shape[2] * self._strength\n",
    "\n",
    "        # Multiply Phi by m (since it was normalized before).\n",
    "        Phi = Phi * m\n",
    "        return mu0, Phi, m, n0\n",
    "\n",
    "    def update(self, X, U):\n",
    "        \"\"\"\n",
    "        Update prior with additional data.\n",
    "        Args:\n",
    "            X: A N x T x dX matrix of sequential state data.\n",
    "            U: A N x T x dU matrix of sequential control data.\n",
    "        \"\"\"\n",
    "        # Constants.\n",
    "        T = X.shape[1] - 1\n",
    "\n",
    "        # Append data to dataset.\n",
    "        if self.X is None:\n",
    "            self.X = X\n",
    "        else:\n",
    "            self.X = np.concatenate([self.X, X], axis=0)\n",
    "\n",
    "        if self.U is None:\n",
    "            self.U = U\n",
    "        else:\n",
    "            self.U = np.concatenate([self.U, U], axis=0)\n",
    "\n",
    "        # Remove excess samples from dataset.\n",
    "        start = max(0, self.X.shape[0] - self._max_samples + 1)\n",
    "        self.X = self.X[start:, :]\n",
    "        self.U = self.U[start:, :]\n",
    "\n",
    "        # Compute cluster dimensionality.\n",
    "        Do = X.shape[2] + U.shape[2] + X.shape[2]  #TODO: Use Xtgt.\n",
    "\n",
    "        # Create dataset.\n",
    "        N = self.X.shape[0]\n",
    "        xux = np.reshape(\n",
    "            np.c_[self.X[:, :T, :], self.U[:, :T, :], self.X[:, 1:(T+1), :]],\n",
    "            [T * N, Do]\n",
    "        )\n",
    "\n",
    "        # Choose number of clusters.\n",
    "        K = int(max(2, min(self._max_clusters,\n",
    "                           np.floor(float(N * T) / self._min_samp))))\n",
    "        LOGGER.debug('Generating %d clusters for dynamics GMM.', K)\n",
    "\n",
    "        # Update GMM.\n",
    "        self.gmm.update(xux, K)\n",
    "\n",
    "    def eval(self, Dx, Du, pts):\n",
    "        \"\"\"\n",
    "        Evaluate prior.\n",
    "        Args:\n",
    "            pts: A N x Dx+Du+Dx matrix.\n",
    "        \"\"\"\n",
    "        # Construct query data point by rearranging entries and adding\n",
    "        # in reference.\n",
    "        assert pts.shape[1] == Dx + Du + Dx\n",
    "\n",
    "        # Perform query and fix mean.\n",
    "        mu0, Phi, m, n0 = self.gmm.inference(pts)\n",
    "\n",
    "        # Factor in multiplier.\n",
    "        n0 = n0 * self._strength\n",
    "        m = m * self._strength\n",
    "\n",
    "        # Multiply Phi by m (since it was normalized before).\n",
    "        Phi *= m\n",
    "        return mu0, Phi, m, n0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression dynamics with an arbitrary prior: `DynamicsLRPrior`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#regularization = 1e-6\n",
    "\n",
    "class DynamicsLRPrior(Dynamics):\n",
    "    \"\"\" Dynamics with linear regression, with arbitrary prior. \"\"\"\n",
    "    def __init__(self, hyperparams):\n",
    "    #def __init__(self):\n",
    "        Dynamics.__init__(self, hyperparams)\n",
    "        #Dynamics.__init__(self)\n",
    "        self.Fm = None\n",
    "        self.fv = None\n",
    "        self.dyn_covar = None        \n",
    "        #self.prior = DynamicsPriorGMM() # Refer to the corresponding class for a deeper understanding\n",
    "        self.prior = self._hyperparams['prior']['type'](self._hyperparams['prior'])\n",
    "\n",
    "    def update_prior(self, samples):\n",
    "        \"\"\" Update dynamics prior. \"\"\"\n",
    "        X = samples.get_X()\n",
    "        U = samples.get_U()\n",
    "        self.prior.update(X, U)\n",
    "\n",
    "    def get_prior(self):\n",
    "        \"\"\" Return the dynamics prior. \"\"\"\n",
    "        return self.prior\n",
    "\n",
    "    #TODO: Merge this with DynamicsLR.fit - lots of duplicated code.\n",
    "    def fit(self, X, U):\n",
    "        \"\"\" Fit dynamics. \"\"\"\n",
    "        N, T, dX = X.shape\n",
    "        dU = U.shape[2]\n",
    "\n",
    "        if N == 1:\n",
    "            raise ValueError(\"Cannot fit dynamics on 1 sample\")\n",
    "\n",
    "        self.Fm = np.zeros([T, dX, dX+dU])\n",
    "        self.fv = np.zeros([T, dX])\n",
    "        self.dyn_covar = np.zeros([T, dX, dX])\n",
    "\n",
    "        it = slice(dX+dU)\n",
    "        ip = slice(dX+dU, dX+dU+dX)\n",
    "        # Fit dynamics with least squares regression.\n",
    "        dwts = (1.0 / N) * np.ones(N)\n",
    "        for t in range(T - 1):\n",
    "            Ys = np.c_[X[:, t, :], U[:, t, :], X[:, t+1, :]]\n",
    "            # Obtain Normal-inverse-Wishart prior.\n",
    "            mu0, Phi, mm, n0 = self.prior.eval(dX, dU, Ys)\n",
    "            sig_reg = np.zeros((dX+dU+dX, dX+dU+dX))\n",
    "            sig_reg[it, it] = self._hyperparams['regularization']\n",
    "            #sig_reg[it, it] = regularization\n",
    "            Fm, fv, dyn_covar = gauss_fit_joint_prior(Ys,\n",
    "                        mu0, Phi, mm, n0, dwts, dX+dU, dX, sig_reg)\n",
    "            self.Fm[t, :, :] = Fm\n",
    "            self.fv[t, :] = fv\n",
    "            self.dyn_covar[t, :, :] = dyn_covar\n",
    "        return self.Fm, self.fv, self.dyn_covar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `BundleType`\n",
    "General utility functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BundleType(object):\n",
    "    \"\"\"\n",
    "    This class bundles many fields, similar to a record or a mutable\n",
    "    namedtuple.\n",
    "    \"\"\"\n",
    "    def __init__(self, variables):\n",
    "        for var, val in variables.items():\n",
    "            object.__setattr__(self, var, val)\n",
    "\n",
    "    # Freeze fields so new ones cannot be set.\n",
    "    def __setattr__(self, key, value):\n",
    "        if not hasattr(self, key):\n",
    "            raise AttributeError(\"%r has no attribute %s\" % (self, key))\n",
    "        object.__setattr__(self, key, value)\n",
    "\n",
    "\n",
    "def check_shape(value, expected_shape, name=''):\n",
    "    \"\"\"\n",
    "    Throws a ValueError if value.shape != expected_shape.\n",
    "    Args:\n",
    "        value: Matrix to shape check.\n",
    "        expected_shape: A tuple or list of integers.\n",
    "        name: An optional name to add to the exception message.\n",
    "    \"\"\"\n",
    "    if value.shape != tuple(expected_shape):\n",
    "        raise ValueError('Shape mismatch %s: Expected %s, got %s' %\n",
    "                         (name, str(expected_shape), str(value.shape)))\n",
    "\n",
    "\n",
    "def finite_differences(func, inputs, func_output_shape=(), epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Computes gradients via finite differences.\n",
    "    derivative = (func(x+epsilon) - func(x-epsilon)) / (2*epsilon)\n",
    "    Args:\n",
    "        func: Function to compute gradient of. Inputs and outputs can be\n",
    "            arbitrary dimension.\n",
    "        inputs: Vector value to compute gradient at.\n",
    "        func_output_shape: Shape of the output of func. Default is\n",
    "            empty-tuple, which works for scalar-valued functions.\n",
    "        epsilon: Difference to use for computing gradient.\n",
    "    Returns:\n",
    "        Gradient vector of each dimension of func with respect to each\n",
    "        dimension of input.\n",
    "    \"\"\"\n",
    "    gradient = np.zeros(inputs.shape+func_output_shape)\n",
    "    for idx, _ in np.ndenumerate(inputs):\n",
    "        test_input = np.copy(inputs)\n",
    "        test_input[idx] += epsilon\n",
    "        obj_d1 = func(test_input)\n",
    "        assert obj_d1.shape == func_output_shape\n",
    "        test_input = np.copy(inputs)\n",
    "        test_input[idx] -= epsilon\n",
    "        obj_d2 = func(test_input)\n",
    "        assert obj_d2.shape == func_output_shape\n",
    "        diff = (obj_d1 - obj_d2) / (2 * epsilon)\n",
    "        gradient[idx] += diff\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `IterationData` and `TrajectoryInfo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IterationData(BundleType):\n",
    "    \"\"\" Collection of iteration variables. \"\"\"\n",
    "    def __init__(self):\n",
    "        variables = {\n",
    "            'sample_list': None,  # List of samples for the current iteration.\n",
    "            'traj_info': None,  # Current TrajectoryInfo object.\n",
    "            'pol_info': None,  # Current PolicyInfo object.\n",
    "            'traj_distr': None,  # Initial trajectory distribution.\n",
    "            'new_traj_distr': None, # Updated trajectory distribution.\n",
    "            'cs': None,  # Sample costs of the current iteration.\n",
    "            'step_mult': 1.0,  # KL step multiplier for the current iteration.\n",
    "            'eta': 1.0,  # Dual variable used in LQR backward pass.\n",
    "        }\n",
    "        BundleType.__init__(self, variables)\n",
    "\n",
    "\n",
    "class TrajectoryInfo(BundleType):\n",
    "    \"\"\" Collection of trajectory-related variables. \"\"\"\n",
    "    def __init__(self):\n",
    "        variables = {\n",
    "            'dynamics': None,  # Dynamics object for the current iteration.\n",
    "            'x0mu': None,  # Mean for the initial state, used by the dynamics.\n",
    "            'x0sigma': None,  # Covariance for the initial state distribution.\n",
    "            'cc': None,  # Cost estimate constant term.\n",
    "            'cv': None,  # Cost estimate vector term.\n",
    "            'Cm': None,  # Cost estimate matrix term.\n",
    "            'last_kl_step': float('inf'),  # KL step of the previous iteration.\n",
    "        }\n",
    "        BundleType.__init__(self, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm superclass: `Algorithm`\n",
    "Base algorithm class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def extract_condition(hyperparams, m):\n",
    "    \"\"\"\n",
    "    Pull the relevant hyperparameters corresponding to the specified\n",
    "    condition, and return a new hyperparameter dictionary.\n",
    "    \"\"\"\n",
    "    return {var: val[m] if isinstance(val, list) else val\n",
    "            for var, val in hyperparams.items()}\n",
    "\n",
    "# Algorithm\n",
    "ALG = {\n",
    "    'inner_iterations': 1,  # Number of iterations.\n",
    "    'min_eta': 1e-5,  # Minimum initial lagrange multiplier in DGD for\n",
    "                      # trajectory optimization.\n",
    "    'kl_step':0.2,\n",
    "    'min_step_mult':0.01,\n",
    "    'max_step_mult':10.0,\n",
    "    'min_mult': 0.1,\n",
    "    'max_mult': 5.0,\n",
    "    # Trajectory settings.\n",
    "    'initial_state_var':1e-6,\n",
    "    'init_traj_distr': None,  # A list of initial LinearGaussianPolicy\n",
    "                              # objects for each condition.\n",
    "    # Trajectory optimization.\n",
    "    'traj_opt': None,\n",
    "    # Weight of maximum entropy term in trajectory optimization.\n",
    "    'max_ent_traj': 0.0,\n",
    "    # Dynamics hyperaparams.\n",
    "    'dynamics': None,\n",
    "    # Costs.\n",
    "    'cost': None,  # A list of Cost objects for each condition.\n",
    "    # Whether or not to sample with neural net policy (only for badmm/mdgps).\n",
    "    'sample_on_policy': False,\n",
    "    # Inidicates if the algorithm requires fitting of the dynamics.\n",
    "    'fit_dynamics': True,    \n",
    "}\n",
    "\n",
    "class Algorithm(object):\n",
    "    \"\"\" Algorithm superclass. \"\"\"\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    def __init__(self, hyperparams):\n",
    "        config = copy.deepcopy(ALG)\n",
    "        config.update(hyperparams)\n",
    "        self._hyperparams = config\n",
    "\n",
    "        if 'train_conditions' in hyperparams:\n",
    "            self._cond_idx = hyperparams['train_conditions']\n",
    "            self.M = len(self._cond_idx)\n",
    "        else:\n",
    "            self.M = hyperparams['conditions']\n",
    "            self._cond_idx = range(self.M)\n",
    "            self._hyperparams['train_conditions'] = self._cond_idx\n",
    "            self._hyperparams['test_conditions'] = self._cond_idx\n",
    "        self.iteration_count = 0\n",
    "\n",
    "        # Grab a few values from the agent.\n",
    "        agent = self._hyperparams['agent']\n",
    "        self.T = self._hyperparams['T'] = agent.T\n",
    "        self.dU = self._hyperparams['dU'] = agent.dU\n",
    "        self.dX = self._hyperparams['dX'] = agent.dX\n",
    "        self.dO = self._hyperparams['dO'] = agent.dO\n",
    "\n",
    "        init_traj_distr = config['init_traj_distr']\n",
    "        init_traj_distr['x0'] = agent.x0\n",
    "        init_traj_distr['dX'] = agent.dX\n",
    "        init_traj_distr['dU'] = agent.dU\n",
    "        del self._hyperparams['agent']  # Don't want to pickle this.\n",
    "\n",
    "        # IterationData objects for each condition.\n",
    "        self.cur = [IterationData() for _ in range(self.M)]\n",
    "        self.prev = [IterationData() for _ in range(self.M)]\n",
    "\n",
    "        if self._hyperparams['fit_dynamics']:\n",
    "            dynamics = self._hyperparams['dynamics']\n",
    "\n",
    "        for m in range(self.M):\n",
    "            self.cur[m].traj_info = TrajectoryInfo()\n",
    "            if self._hyperparams['fit_dynamics']:\n",
    "                self.cur[m].traj_info.dynamics = dynamics['type'](dynamics)\n",
    "            init_traj_distr = extract_condition(\n",
    "                self._hyperparams['init_traj_distr'], self._cond_idx[m]\n",
    "            )\n",
    "            self.cur[m].traj_distr = init_traj_distr['type'](init_traj_distr)\n",
    "\n",
    "        self.traj_opt = hyperparams['traj_opt']['type'](\n",
    "            hyperparams['traj_opt']\n",
    "        )\n",
    "        if type(hyperparams['cost']) == list:\n",
    "            self.cost = [\n",
    "                hyperparams['cost'][i]['type'](hyperparams['cost'][i])\n",
    "                for i in range(self.M)\n",
    "            ]\n",
    "        else:\n",
    "            self.cost = [\n",
    "                hyperparams['cost']['type'](hyperparams['cost'])\n",
    "                for _ in range(self.M)\n",
    "            ]\n",
    "        self.base_kl_step = self._hyperparams['kl_step']\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def iteration(self, sample_list):\n",
    "        \"\"\" Run iteration of the algorithm. \"\"\"\n",
    "        raise NotImplementedError(\"Must be implemented in subclass\")\n",
    "\n",
    "    def _update_dynamics(self):\n",
    "        \"\"\"\n",
    "        Instantiate dynamics objects and update prior. Fit dynamics to\n",
    "        current samples.\n",
    "        \"\"\"\n",
    "        for m in range(self.M):\n",
    "            cur_data = self.cur[m].sample_list\n",
    "            X = cur_data.get_X()\n",
    "            U = cur_data.get_U()\n",
    "\n",
    "            # Update prior and fit dynamics.\n",
    "            self.cur[m].traj_info.dynamics.update_prior(cur_data)\n",
    "            self.cur[m].traj_info.dynamics.fit(X, U)\n",
    "\n",
    "            # Fit x0mu/x0sigma.\n",
    "            x0 = X[:, 0, :]\n",
    "            x0mu = np.mean(x0, axis=0)\n",
    "            self.cur[m].traj_info.x0mu = x0mu\n",
    "            self.cur[m].traj_info.x0sigma = np.diag(\n",
    "                np.maximum(np.var(x0, axis=0),\n",
    "                           self._hyperparams['initial_state_var'])\n",
    "            )\n",
    "\n",
    "            prior = self.cur[m].traj_info.dynamics.get_prior()\n",
    "            if prior:\n",
    "                mu0, Phi, priorm, n0 = prior.initial_state()\n",
    "                N = len(cur_data)\n",
    "                self.cur[m].traj_info.x0sigma += \\\n",
    "                        Phi + (N*priorm) / (N+priorm) * \\\n",
    "                        np.outer(x0mu-mu0, x0mu-mu0) / (N+n0)\n",
    "\n",
    "    def _update_trajectories(self):\n",
    "        \"\"\"\n",
    "        Compute new linear Gaussian controllers.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'new_traj_distr'):\n",
    "            self.new_traj_distr = [\n",
    "                self.cur[cond].traj_distr for cond in range(self.M)\n",
    "            ]\n",
    "        for cond in range(self.M):\n",
    "            self.new_traj_distr[cond], self.cur[cond].eta = \\\n",
    "                    self.traj_opt.update(cond, self)\n",
    "\n",
    "    def _eval_cost(self, cond):\n",
    "        \"\"\"\n",
    "        Evaluate costs for all samples for a condition.\n",
    "        Args:\n",
    "            cond: Condition to evaluate cost on.\n",
    "        \"\"\"\n",
    "        # Constants.\n",
    "        T, dX, dU = self.T, self.dX, self.dU\n",
    "        N = len(self.cur[cond].sample_list)\n",
    "\n",
    "        # Compute cost.\n",
    "        cs = np.zeros((N, T))\n",
    "        cc = np.zeros((N, T))\n",
    "        cv = np.zeros((N, T, dX+dU))\n",
    "        Cm = np.zeros((N, T, dX+dU, dX+dU))\n",
    "        for n in range(N):\n",
    "            sample = self.cur[cond].sample_list[n]\n",
    "            # Get costs.\n",
    "            l, lx, lu, lxx, luu, lux = self.cost[cond].eval(sample)\n",
    "            cc[n, :] = l\n",
    "            cs[n, :] = l\n",
    "\n",
    "            # Assemble matrix and vector.\n",
    "            cv[n, :, :] = np.c_[lx, lu]\n",
    "            Cm[n, :, :, :] = np.concatenate(\n",
    "                (np.c_[lxx, np.transpose(lux, [0, 2, 1])], np.c_[lux, luu]),\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Adjust for expanding cost around a sample.\n",
    "            X = sample.get_X()\n",
    "            U = sample.get_U()\n",
    "            yhat = np.c_[X, U]\n",
    "            rdiff = -yhat\n",
    "            rdiff_expand = np.expand_dims(rdiff, axis=2)\n",
    "            cv_update = np.sum(Cm[n, :, :, :] * rdiff_expand, axis=1)\n",
    "            cc[n, :] += np.sum(rdiff * cv[n, :, :], axis=1) + 0.5 * \\\n",
    "                    np.sum(rdiff * cv_update, axis=1)\n",
    "            cv[n, :, :] += cv_update\n",
    "\n",
    "        # Fill in cost estimate.\n",
    "        self.cur[cond].traj_info.cc = np.mean(cc, 0)  # Constant term (scalar).\n",
    "        self.cur[cond].traj_info.cv = np.mean(cv, 0)  # Linear term (vector).\n",
    "        self.cur[cond].traj_info.Cm = np.mean(Cm, 0)  # Quadratic term (matrix).\n",
    "\n",
    "        self.cur[cond].cs = cs  # True value of cost.\n",
    "\n",
    "    def _advance_iteration_variables(self):\n",
    "        \"\"\"\n",
    "        Move all 'cur' variables to 'prev', and advance iteration\n",
    "        counter.\n",
    "        \"\"\"\n",
    "        self.iteration_count += 1\n",
    "        self.prev = copy.deepcopy(self.cur)\n",
    "        # TODO: change IterationData to reflect new stuff better\n",
    "        for m in range(self.M):\n",
    "            self.prev[m].new_traj_distr = self.new_traj_distr[m]\n",
    "        self.cur = [IterationData() for _ in range(self.M)]\n",
    "        for m in range(self.M):\n",
    "            self.cur[m].traj_info = TrajectoryInfo()\n",
    "            self.cur[m].traj_info.dynamics = copy.deepcopy(self.prev[m].traj_info.dynamics)\n",
    "            self.cur[m].step_mult = self.prev[m].step_mult\n",
    "            self.cur[m].eta = self.prev[m].eta\n",
    "            self.cur[m].traj_distr = self.new_traj_distr[m]\n",
    "        delattr(self, 'new_traj_distr')\n",
    "\n",
    "    def _set_new_mult(self, predicted_impr, actual_impr, m):\n",
    "        \"\"\"\n",
    "        Adjust step size multiplier according to the predicted versus\n",
    "        actual improvement.\n",
    "        \"\"\"\n",
    "        # Model improvement as I = predicted_dI * KL + penalty * KL^2,\n",
    "        # where predicted_dI = pred/KL and penalty = (act-pred)/(KL^2).\n",
    "        # Optimize I w.r.t. KL: 0 = predicted_dI + 2 * penalty * KL =>\n",
    "        # KL' = (-predicted_dI)/(2*penalty) = (pred/2*(pred-act)) * KL.\n",
    "        # Therefore, the new multiplier is given by pred/2*(pred-act).\n",
    "        new_mult = predicted_impr / (2.0 * max(1e-4,\n",
    "                                               predicted_impr - actual_impr))\n",
    "        new_mult = max(0.1, min(5.0, new_mult))\n",
    "        new_step = max(\n",
    "            min(new_mult * self.cur[m].step_mult,\n",
    "                self._hyperparams['max_step_mult']),\n",
    "            self._hyperparams['min_step_mult']\n",
    "        )\n",
    "        self.cur[m].step_mult = new_step\n",
    "\n",
    "        if new_mult > 1:\n",
    "            LOGGER.debug('Increasing step size multiplier to %f', new_step)\n",
    "        else:\n",
    "            LOGGER.debug('Decreasing step size multiplier to %f', new_step)\n",
    "\n",
    "    def _measure_ent(self, m):\n",
    "        \"\"\" Measure the entropy of the current trajectory. \"\"\"\n",
    "        ent = 0\n",
    "        for t in range(self.T):\n",
    "            ent = ent + np.sum(\n",
    "                np.log(np.diag(self.cur[m].traj_distr.chol_pol_covar[t, :, :]))\n",
    "            )\n",
    "        return ent\n",
    "\n",
    "    # For pickling.\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['_random_state'] = random.getstate()\n",
    "        state['_np_random_state'] = np.random.get_state()\n",
    "        return state\n",
    "\n",
    "    # For unpickling.\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__ = state\n",
    "        random.setstate(state.pop('_random_state'))\n",
    "        np.random.set_state(state.pop('_np_random_state'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iLQG-based trajectory optimization: `AlgorithmTrajOpt`\n",
    "Sample-based trajectory optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AlgorithmTrajOpt(Algorithm):\n",
    "    \"\"\" Sample-based trajectory optimization. \"\"\"\n",
    "    def __init__(self, hyperparams):\n",
    "        Algorithm.__init__(self, hyperparams)\n",
    "\n",
    "    def iteration(self, sample_lists):\n",
    "        \"\"\"\n",
    "        Run iteration of LQR.\n",
    "        Args:\n",
    "            sample_lists: List of SampleList objects for each condition.\n",
    "        \"\"\"\n",
    "        for m in range(self.M):\n",
    "            self.cur[m].sample_list = sample_lists[m]\n",
    "\n",
    "        # Update dynamics model using all samples.\n",
    "        self._update_dynamics()\n",
    "\n",
    "        self._update_step_size()  # KL Divergence step size.\n",
    "\n",
    "        # Run inner loop to compute new policies.\n",
    "        for _ in range(self._hyperparams['inner_iterations']):\n",
    "            self._update_trajectories()\n",
    "\n",
    "        self._advance_iteration_variables()\n",
    "\n",
    "    def _update_step_size(self):\n",
    "        \"\"\" Evaluate costs on samples, and adjust the step size. \"\"\"\n",
    "        # Evaluate cost function for all conditions and samples.\n",
    "        for m in range(self.M):\n",
    "            self._eval_cost(m)\n",
    "\n",
    "        # Adjust step size relative to the previous iteration.\n",
    "        for m in range(self.M):\n",
    "            if self.iteration_count >= 1 and self.prev[m].sample_list:\n",
    "                self._stepadjust(m)\n",
    "\n",
    "    def _stepadjust(self, m):\n",
    "        \"\"\"\n",
    "        Calculate new step sizes.\n",
    "        Args:\n",
    "            m: Condition\n",
    "        \"\"\"\n",
    "        # Compute values under Laplace approximation. This is the policy\n",
    "        # that the previous samples were actually drawn from under the\n",
    "        # dynamics that were estimated from the previous samples.\n",
    "        previous_laplace_obj = self.traj_opt.estimate_cost(\n",
    "            self.prev[m].traj_distr, self.prev[m].traj_info\n",
    "        )\n",
    "        # This is the policy that we just used under the dynamics that\n",
    "        # were estimated from the previous samples (so this is the cost\n",
    "        # we thought we would have).\n",
    "        new_predicted_laplace_obj = self.traj_opt.estimate_cost(\n",
    "            self.cur[m].traj_distr, self.prev[m].traj_info\n",
    "        )\n",
    "\n",
    "        # This is the actual cost we have under the current trajectory\n",
    "        # based on the latest samples.\n",
    "        new_actual_laplace_obj = self.traj_opt.estimate_cost(\n",
    "            self.cur[m].traj_distr, self.cur[m].traj_info\n",
    "        )\n",
    "\n",
    "        # Measure the entropy of the current trajectory (for printout).\n",
    "        ent = self._measure_ent(m)\n",
    "\n",
    "        # Compute actual objective values based on the samples.\n",
    "        previous_mc_obj = np.mean(np.sum(self.prev[m].cs, axis=1), axis=0)\n",
    "        new_mc_obj = np.mean(np.sum(self.cur[m].cs, axis=1), axis=0)\n",
    "\n",
    "        LOGGER.debug('Trajectory step: ent: %f cost: %f -> %f',\n",
    "                     ent, previous_mc_obj, new_mc_obj)\n",
    "\n",
    "        # Compute predicted and actual improvement.\n",
    "        predicted_impr = np.sum(previous_laplace_obj) - \\\n",
    "                np.sum(new_predicted_laplace_obj)\n",
    "        actual_impr = np.sum(previous_laplace_obj) - \\\n",
    "                np.sum(new_actual_laplace_obj)\n",
    "\n",
    "        # Print improvement details.\n",
    "        LOGGER.debug('Previous cost: Laplace: %f MC: %f',\n",
    "                     np.sum(previous_laplace_obj), previous_mc_obj)\n",
    "        LOGGER.debug('Predicted new cost: Laplace: %f MC: %f',\n",
    "                     np.sum(new_predicted_laplace_obj), new_mc_obj)\n",
    "        LOGGER.debug('Actual new cost: Laplace: %f MC: %f',\n",
    "                     np.sum(new_actual_laplace_obj), new_mc_obj)\n",
    "        LOGGER.debug('Predicted/actual improvement: %f / %f',\n",
    "                     predicted_impr, actual_impr)\n",
    "\n",
    "        self._set_new_mult(predicted_impr, actual_impr, m)\n",
    "\n",
    "    def compute_costs(self, m, eta, augment=True):\n",
    "        \"\"\" Compute cost estimates used in the LQR backward pass. \"\"\"\n",
    "        traj_info, traj_distr = self.cur[m].traj_info, self.cur[m].traj_distr\n",
    "        if not augment:  # Whether to augment cost with term to penalize KL\n",
    "            return traj_info.Cm, traj_info.cv\n",
    "\n",
    "        multiplier = self._hyperparams['max_ent_traj']\n",
    "        fCm, fcv = traj_info.Cm / (eta + multiplier), traj_info.cv / (eta + multiplier)\n",
    "        K, ipc, k = traj_distr.K, traj_distr.inv_pol_covar, traj_distr.k\n",
    "\n",
    "        # Add in the trajectory divergence term.\n",
    "        for t in range(self.T - 1, -1, -1):\n",
    "            fCm[t, :, :] += eta / (eta + multiplier) * np.vstack([\n",
    "                np.hstack([\n",
    "                    K[t, :, :].T.dot(ipc[t, :, :]).dot(K[t, :, :]),\n",
    "                    -K[t, :, :].T.dot(ipc[t, :, :])\n",
    "                ]),\n",
    "                np.hstack([\n",
    "                    -ipc[t, :, :].dot(K[t, :, :]), ipc[t, :, :]\n",
    "                ])\n",
    "            ])\n",
    "            fcv[t, :] += eta / (eta + multiplier) * np.hstack([\n",
    "                K[t, :, :].T.dot(ipc[t, :, :]).dot(k[t, :]),\n",
    "                -ipc[t, :, :].dot(k[t, :])\n",
    "            ])\n",
    "\n",
    "        return fCm, fcv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Instantiate and process the corresponding hyperparameters. Using the arm example hyperparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'JOINT_ANGLES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-075c329b41b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m SENSOR_DIMS = {\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mJOINT_ANGLES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mJOINT_VELOCITIES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mEND_EFFECTOR_POINTS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'JOINT_ANGLES' is not defined"
     ]
    }
   ],
   "source": [
    "## TODO!! Review\n",
    "\n",
    "\n",
    "SENSOR_DIMS = {\n",
    "    JOINT_ANGLES: 2,\n",
    "    JOINT_VELOCITIES: 2,\n",
    "    END_EFFECTOR_POINTS: 3,\n",
    "    ACTION: 2\n",
    "}\n",
    "\n",
    "BASE_DIR = '/'.join(str.split(gps_filepath, '/')[:-2])\n",
    "EXP_DIR = BASE_DIR + '/../experiments/box2d_arm_example/'\n",
    "\n",
    "\n",
    "common = {\n",
    "    'experiment_name': 'box2d_arm_example' + '_' + \\\n",
    "            datetime.strftime(datetime.now(), '%m-%d-%y_%H-%M'),\n",
    "    'experiment_dir': EXP_DIR,\n",
    "    'data_files_dir': EXP_DIR + 'data_files/',\n",
    "    'log_filename': EXP_DIR + 'log.txt',\n",
    "    'conditions': 1,\n",
    "}\n",
    "\n",
    "if not os.path.exists(common['data_files_dir']):\n",
    "    os.makedirs(common['data_files_dir'])\n",
    "\n",
    "agent = {\n",
    "    'type': AgentBox2D,\n",
    "    'target_state' : np.array([0, 0]),\n",
    "    'world' : ArmWorld,\n",
    "    'render' : False,\n",
    "    'x0': np.array([0.75*np.pi, 0.5*np.pi, 0, 0, 0, 0, 0]),\n",
    "    'rk': 0,\n",
    "    'dt': 0.05,\n",
    "    'substeps': 1,\n",
    "    'conditions': common['conditions'],\n",
    "    'pos_body_idx': np.array([]),\n",
    "    'pos_body_offset': np.array([]),\n",
    "    'T': 100,\n",
    "    'sensor_dims': SENSOR_DIMS,\n",
    "    'state_include': [JOINT_ANGLES, JOINT_VELOCITIES, END_EFFECTOR_POINTS],\n",
    "    'obs_include': [],\n",
    "}\n",
    "\n",
    "algorithm = {\n",
    "    'type': AlgorithmTrajOpt,\n",
    "    'conditions': common['conditions'],\n",
    "}\n",
    "\n",
    "algorithm['init_traj_distr'] = {\n",
    "    'type': init_lqr,\n",
    "    'init_gains': np.zeros(SENSOR_DIMS[ACTION]),\n",
    "    'init_acc': np.zeros(SENSOR_DIMS[ACTION]),\n",
    "    'init_var': 0.1,\n",
    "    'stiffness': 0.01,\n",
    "    'dt': agent['dt'],\n",
    "    'T': agent['T'],\n",
    "}\n",
    "\n",
    "action_cost = {\n",
    "    'type': CostAction,\n",
    "    'wu': np.array([1, 1])\n",
    "}\n",
    "\n",
    "state_cost = {\n",
    "    'type': CostState,\n",
    "    'data_types' : {\n",
    "        JOINT_ANGLES: {\n",
    "            'wp': np.array([1, 1]),\n",
    "            'target_state': agent[\"target_state\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "algorithm['cost'] = {\n",
    "    'type': CostSum,\n",
    "    'costs': [action_cost, state_cost],\n",
    "    'weights': [1e-5, 1.0],\n",
    "}\n",
    "\n",
    "algorithm['dynamics'] = {\n",
    "    'type': DynamicsLRPrior,\n",
    "    'regularization': 1e-6,\n",
    "    'prior': {\n",
    "        'type': DynamicsPriorGMM,\n",
    "        'max_clusters': 20,\n",
    "        'min_samples_per_cluster': 40,\n",
    "        'max_samples': 20,\n",
    "    },\n",
    "}\n",
    "\n",
    "algorithm['traj_opt'] = {\n",
    "    'type': TrajOptLQRPython,\n",
    "}\n",
    "\n",
    "algorithm['policy_opt'] = {}\n",
    "\n",
    "config = {\n",
    "    'iterations': 10,\n",
    "    'num_samples': 5,\n",
    "    'verbose_trials': 5,\n",
    "    'common': common,\n",
    "    'agent': agent,\n",
    "    'gui_on': True,\n",
    "    'algorithm': algorithm,\n",
    "}\n",
    "\n",
    "common['info'] = generate_experiment_info(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
